{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1352cc5a-501d-47c2-a661-65b148acefd4",
   "metadata": {},
   "source": [
    "# 自定义 policy 模块中的 feature_extractor 和 mlp_extractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8429fc6e-8178-4d8d-be05-1358a1ca36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3967887e-c7ae-483a-83e7-47d2a7202828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\pyvenv\\rl\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.1     |\n",
      "|    ep_rew_mean     | 22.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 368      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 31.7        |\n",
      "|    ep_rew_mean          | 31.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015045412 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.683      |\n",
      "|    explained_variance   | 0.000748    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.69        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    value_loss           | 24.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 43.3        |\n",
      "|    ep_rew_mean          | 43.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 261         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009033401 |\n",
      "|    clip_fraction        | 0.0382      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.661      |\n",
      "|    explained_variance   | 0.037       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.6        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 62.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 56.1        |\n",
      "|    ep_rew_mean          | 56.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008552659 |\n",
      "|    clip_fraction        | 0.0532      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.636      |\n",
      "|    explained_variance   | 0.0432      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 71.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 69.1         |\n",
      "|    ep_rew_mean          | 69.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 42           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073887673 |\n",
      "|    clip_fraction        | 0.0507       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.613       |\n",
      "|    explained_variance   | 0.0903       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.5         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0145      |\n",
      "|    value_loss           | 79.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x21b2e811a90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 定义自定义特征提取器 和 简单的 mlp_extractor 自定义\n",
    "class CustomCombinedExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_input_channels, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, features_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.layers(observations)\n",
    "\n",
    "# 2. 配置 policy_kwargs\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCombinedExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=256),\n",
    "    # 自定义 mlp_extractor 就在这里：\n",
    "    # net_arch=dict(\n",
    "    #     pi=[128, 64],  # 策略网络 (Actor) 的隐藏层：256 -> 128 -> 64\n",
    "    #     vf=[64, 64]    # 价值网络 (Critic) 的隐藏层：256 -> 64 -> 64\n",
    "    # )\n",
    ")\n",
    "\n",
    "# 3. 创建模型\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7aea6ea-d8d1-4c9a-a270-2d64b8e333a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 状态测试 ---\n",
      "输入状态: [-1.3300587  0.816699   0.0824696  1.2880276]\n",
      "动作概率: 向左(0): 14.47%, 向右(1): 85.53%\n",
      "最终决策: 1\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "# 1. 采样状态\n",
    "random_obs = env.observation_space.sample()\n",
    "\n",
    "# 2. 准备 Tensor 并确保设备一致\n",
    "# model.device 会自动返回 'cuda' 或 'cpu'\n",
    "obs_tensor = th.as_tensor(random_obs).float().unsqueeze(0).to(model.device)\n",
    "\n",
    "# 3. 手动推断分布\n",
    "with th.no_grad():\n",
    "    # 获取动作分布\n",
    "    dist = model.policy.get_distribution(obs_tensor)\n",
    "    \n",
    "    # 提取概率（转回 CPU 以便 numpy/print 处理）\n",
    "    probs = dist.distribution.probs.cpu().numpy()[0]\n",
    "    \n",
    "    print(f\"--- 状态测试 ---\")\n",
    "    print(f\"输入状态: {random_obs}\")\n",
    "    print(f\"动作概率: 向左(0): {probs[0]:.2%}, 向右(1): {probs[1]:.2%}\")\n",
    "    \n",
    "    # 选出概率最大的动作\n",
    "    action = np.argmax(probs)\n",
    "    print(f\"最终决策: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0019ccb8-77e3-4f20-93ae-4a90a8a22b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.3300587,  0.816699 ,  0.0824696,  1.2880276], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adcd9ffc-b938-4456-81e1-4a34203a7844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): CustomCombinedExtractor(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (pi_features_extractor): CustomCombinedExtractor(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (vf_features_extractor): CustomCombinedExtractor(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b98e206-47ba-4be7-9a34-33cb48c053bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2315767588112, 2315767588112, 2315767588112)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(model.policy.features_extractor), id(model.policy.vf_features_extractor), id(model.policy.pi_features_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3c150e7-a915-46d0-9f26-0ac43cd7068f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# 1. 定义自定义特征提取器 和 mlp_extractor 自定义\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "# 1. 定义一个完全自定义的 MlpExtractor 类\n",
    "class MyCustomMlpExtractor(nn.Module):\n",
    "    def __init__(self, feature_dim: int):\n",
    "        super().__init__()\n",
    "        # 定义输出维度，必须告知 Policy 最终输出给 action_net 的维度是多少\n",
    "        self.latent_dim_pi = 64\n",
    "        self.latent_dim_vf = 64\n",
    "\n",
    "        # 策略网络分支：加入 Dropout 层作为示例\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(128, self.latent_dim_pi),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 价值网络分支\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 128),\n",
    "            nn.Tanh(), # 甚至可以在这里用不同的激活函数\n",
    "            nn.Linear(128, self.latent_dim_vf),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, features: th.Tensor):\n",
    "        # 返回 (policy_latent, value_latent)\n",
    "        return self.policy_net(features), self.value_net(features)\n",
    "\n",
    "    # 为了兼容性，SB3 需要这两个方法\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)\n",
    "\n",
    "# 2. 定义一个新的 Policy 类来使用这个 Extractor\n",
    "class CustomPolicy(ActorCriticPolicy):\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        # 这里用我们自定义的类替换默认的 MlpExtractor\n",
    "        self.mlp_extractor = MyCustomMlpExtractor(self.features_dim)\n",
    "\n",
    "# 3. 使用这个自定义 Policy\n",
    "model = PPO(CustomPolicy, env, policy_kwargs=policy_kwargs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef4e38eb-239f-409b-941b-6baf54f0af21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomPolicy(\n",
       "  (features_extractor): CustomCombinedExtractor(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (pi_features_extractor): CustomCombinedExtractor(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (vf_features_extractor): CustomCombinedExtractor(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (mlp_extractor): MyCustomMlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (4): ReLU()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6bfa43-e8f4-48a7-ab67-32242c3781ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
